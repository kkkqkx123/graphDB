# 性能优化方案

## 概述
graphDB当前实现优先考虑功能完整性，但在性能方面仍有很大提升空间。本文档描述了性能优化的方案。

## 需求分析

### 性能瓶颈
1. 缺少查询执行计划缓存
2. 缺少批量操作支持
3. 索引利用不足
4. 内存使用效率不高
5. 并发处理能力有限

## 详细方案

### 1. 查询计划缓存

#### 文件: `src/query/planner/cache.rs`

```rust
use std::collections::HashMap;
use std::sync::{Arc, RwLock};
use lru::LruCache;
use crate::core::Value;
use crate::query::ast::Statement;

#[derive(Debug, Clone, Hash, PartialEq, Eq)]
pub struct QuerySignature {
    pub query: String,
    pub parameters: Vec<Value>,
}

#[derive(Debug, Clone)]
pub struct CachedQueryPlan {
    pub plan: crate::query::planner::QueryPlan,
    pub created_at: std::time::SystemTime,
    pub hit_count: usize,
    pub last_accessed: std::time::SystemTime,
}

pub struct QueryPlanCache {
    cache: Arc<RwLock<LruCache<QuerySignature, CachedQueryPlan>>>,
    max_cache_size: usize,
}

impl QueryPlanCache {
    pub fn new(max_cache_size: usize) -> Self {
        Self {
            cache: Arc::new(RwLock::new(LruCache::new(max_cache_size))),
            max_cache_size,
        }
    }

    pub fn get(&self, signature: &QuerySignature) -> Option<CachedQueryPlan> {
        let mut cache = self.cache.write().unwrap();
        cache.get(signature).cloned()
    }

    pub fn insert(&self, signature: QuerySignature, plan: crate::query::planner::QueryPlan) {
        let mut cache = self.cache.write().unwrap();
        
        let cached_plan = CachedQueryPlan {
            plan,
            created_at: std::time::SystemTime::now(),
            hit_count: 0,
            last_accessed: std::time::SystemTime::now(),
        };
        
        cache.put(signature, cached_plan);
    }

    pub fn update_access(&self, signature: &QuerySignature) {
        let mut cache = self.cache.write().unwrap();
        if let Some(mut plan) = cache.get_mut(signature) {
            plan.hit_count += 1;
            plan.last_accessed = std::time::SystemTime::now();
        }
    }

    pub fn clear(&self) {
        let mut cache = self.cache.write().unwrap();
        cache.clear();
    }

    pub fn stats(&self) -> QueryPlanCacheStats {
        let cache = self.cache.read().unwrap();
        QueryPlanCacheStats {
            hits: 0, // 这里需要维护命中计数
            misses: 0, // 这里需要维护未命中计数
            size: cache.len(),
            capacity: cache.capacit(),
        }
    }
}

pub struct QueryPlanCacheStats {
    pub hits: usize,
    pub misses: usize,
    pub size: usize,
    pub capacity: usize,
}

impl Default for QueryPlanCache {
    fn default() -> Self {
        Self::new(1000) // 默认缓存1000个计划
    }
}
```

#### 文件: `src/query/planner/mod.rs` （增强版本）

```rust
use super::cache::{QueryPlanCache, QuerySignature};
use std::sync::Arc;

pub struct EnhancedQueryPlanner<S: crate::storage::StorageEngine> {
    storage: Arc<std::sync::Mutex<S>>,
    cache: Arc<QueryPlanCache>,
}

impl<S: crate::storage::StorageEngine> EnhancedQueryPlanner<S> {
    pub fn new(storage: Arc<std::sync::Mutex<S>>, cache_size: usize) -> Self {
        Self {
            storage,
            cache: Arc::new(QueryPlanCache::new(cache_size)),
        }
    }

    pub fn plan(&self, query: &str, parameters: &[Value]) -> Result<QueryPlan, crate::query::QueryError> {
        let signature = QuerySignature {
            query: query.to_string(),
            parameters: parameters.to_vec(),
        };

        // 尝试从缓存中获取计划
        if let Some(mut cached_plan) = self.cache.get(&signature) {
            self.cache.update_access(&signature);
            return Ok(cached_plan.plan);
        }

        // 如果未缓存，创建新计划
        let plan = self.create_plan(query, parameters)?;
        
        // 将新计划存入缓存
        self.cache.insert(signature, plan.clone());
        
        Ok(plan)
    }

    fn create_plan(&self, query: &str, parameters: &[Value]) -> Result<QueryPlan, crate::query::QueryError> {
        // 原有的计划创建逻辑
        todo!()
    }
}
```

### 2. 批量操作优化

#### 文件: `src/storage/batch.rs`

```rust
use crate::core::{Vertex, Edge, Value};
use super::{StorageEngine, StorageError};

pub struct BatchOperation {
    pub operations: Vec<BatchOp>,
}

#[derive(Debug)]
pub enum BatchOp {
    InsertNode(Vertex),
    UpdateNode(Vertex),
    DeleteNode(Value),
    InsertEdge(Edge),
    DeleteEdge(Value, Value, String),
}

impl BatchOperation {
    pub fn new() -> Self {
        Self {
            operations: Vec::new(),
        }
    }

    pub fn insert_node(&mut self, vertex: Vertex) -> &mut Self {
        self.operations.push(BatchOp::InsertNode(vertex));
        self
    }

    pub fn update_node(&mut self, vertex: Vertex) -> &mut Self {
        self.operations.push(BatchOp::UpdateNode(vertex));
        self
    }

    pub fn delete_node(&mut self, id: Value) -> &mut Self {
        self.operations.push(BatchOp::DeleteNode(id));
        self
    }

    pub fn insert_edge(&mut self, edge: Edge) -> &mut Self {
        self.operations.push(BatchOp::InsertEdge(edge));
        self
    }

    pub fn delete_edge(&mut self, src: Value, dst: Value, edge_type: String) -> &mut Self {
        self.operations.push(BatchOp::DeleteEdge(src, dst, edge_type));
        self
    }
}

pub trait BatchStorageEngine: StorageEngine {
    fn execute_batch(&mut self, batch: BatchOperation) -> Result<(), StorageError>;
}

impl<S: StorageEngine> BatchStorageEngine for S {
    fn execute_batch(&mut self, batch: BatchOperation) -> Result<(), StorageError> {
        // 开始一个事务
        let tx_id = self.begin_transaction()?;
        
        // 执行批量操作
        for op in batch.operations {
            match op {
                BatchOp::InsertNode(vertex) => self.insert_node(vertex)?,
                BatchOp::UpdateNode(vertex) => self.update_node(vertex)?,
                BatchOp::DeleteNode(id) => self.delete_node(&id)?,
                BatchOp::InsertEdge(edge) => self.insert_edge(edge)?,
                BatchOp::DeleteEdge(src, dst, edge_type) => self.delete_edge(&src, &dst, &edge_type)?,
            }
        }
        
        // 提交事务
        self.commit_transaction(tx_id)?;
        
        Ok(())
    }
}
```

### 3. 内存池优化

#### 文件: `src/core/memory_pool.rs`

```rust
use std::sync::{Arc, Mutex};
use std::collections::VecDeque;
use std::ptr;
use std::mem;

pub struct MemoryPool<T> {
    pool: Arc<Mutex<VecDeque<T>>>,
    max_size: usize,
}

impl<T: Default + Clone> MemoryPool<T> {
    pub fn new(max_size: usize) -> Self {
        Self {
            pool: Arc::new(Mutex::new(VecDeque::with_capacity(max_size))),
            max_size,
        }
    }

    pub fn alloc(&self) -> T {
        let mut pool = self.pool.lock().unwrap();
        
        if let Some(item) = pool.pop_front() {
            item
        } else {
            T::default()
        }
    }

    pub fn dealloc(&self, item: T) {
        let mut pool = self.pool.lock().unwrap();
        
        if pool.len() < self.max_size {
            pool.push_back(item);
        }
    }
}

pub struct PooledVec<T> {
    data: Vec<T>,
    pool: Option<Arc<MemoryPool<Vec<T>>>>,
}

impl<T> PooledVec<T> {
    pub fn new() -> Self {
        Self {
            data: Vec::new(),
            pool: None,
        }
    }

    pub fn with_pool(pool: Arc<MemoryPool<Vec<T>>>) -> Self {
        Self {
            data: pool.alloc(),
            pool: Some(pool),
        }
    }

    pub fn push(&mut self, value: T) {
        self.data.push(value);
    }

    pub fn clear(&mut self) {
        self.data.clear();
    }

    pub fn len(&self) -> usize {
        self.data.len()
    }

    pub fn is_empty(&self) -> bool {
        self.data.is_empty()
    }

    pub fn iter(&self) -> std::slice::Iter<T> {
        self.data.iter()
    }
}

impl<T> Drop for PooledVec<T> {
    fn drop(&mut self) {
        if let Some(pool) = &self.pool {
            pool.dealloc(std::mem::take(&mut self.data));
        }
    }
}
```

### 4. 并发优化

#### 文件: `src/core/concurrent.rs`

```rust
use std::sync::{Arc, RwLock, Mutex, atomic::{AtomicUsize, AtomicBool, Ordering}};
use std::collections::HashMap;
use std::hash::Hash;

pub struct ConcurrentLRUCache<K, V> {
    cache: RwLock<lru::LruCache<K, V>>,
    max_size: usize,
    stats: CacheStats,
}

#[derive(Default)]
struct CacheStats {
    hits: AtomicUsize,
    misses: AtomicUsize,
    updates: AtomicUsize,
}

impl<K, V> ConcurrentLRUCache<K, V>
where
    K: Hash + Eq + Clone,
    V: Clone,
{
    pub fn new(max_size: usize) -> Self {
        Self {
            cache: RwLock::new(lru::LruCache::new(max_size)),
            max_size,
            stats: CacheStats::default(),
        }
    }

    pub fn get(&self, key: &K) -> Option<V> {
        let mut cache = self.cache.write().unwrap(); // 使用写锁以便进行LRU更新
        if let Some(value) = cache.get(key) {
            self.stats.hits.fetch_add(1, Ordering::SeqCst);
            Some(value.clone())
        } else {
            self.stats.misses.fetch_add(1, Ordering::SeqCst);
            None
        }
    }

    pub fn put(&self, key: K, value: V) {
        let mut cache = self.cache.write().unwrap();
        cache.put(key, value);
        self.stats.updates.fetch_add(1, Ordering::SeqCst);
    }

    pub fn remove(&self, key: &K) -> Option<V> {
        let mut cache = self.cache.write().unwrap();
        cache.pop(key)
    }

    pub fn stats(&self) -> (usize, usize, usize) {
        (
            self.stats.hits.load(Ordering::SeqCst),
            self.stats.misses.load(Ordering::SeqCst),
            self.stats.updates.load(Ordering::SeqCst),
        )
    }
}

pub struct ThreadPool {
    workers: Vec<std::thread::JoinHandle<()>>,
    sender: std::sync::mpsc::Sender<Job>,
}

type Job = Box<dyn FnOnce() + Send + 'static>;

impl ThreadPool {
    pub fn new(size: usize) -> Self {
        assert!(size > 0);

        let (sender, receiver) = std::sync::mpsc::channel();
        let receiver = Arc::new(std::sync::Mutex::new(receiver));

        let mut workers = Vec::with_capacity(size);

        for id in 0..size {
            workers.push(Worker::new(id, Arc::clone(&receiver)));
        }

        Self { workers, sender }
    }

    pub fn execute<F>(&self, f: F) -> Result<(), ()>
    where
        F: FnOnce() + Send + 'static,
    {
        let job = Box::new(f);

        self.sender.send(job).map_err(|_| ())
    }
}

impl Drop for ThreadPool {
    fn drop(&mut self) {
        drop(&self.sender);

        for worker in &mut self.workers {
            if let Some(thread) = worker.thread.take() {
                thread.join().unwrap();
            }
        }
    }
}

struct Worker {
    id: usize,
    thread: Option<std::thread::JoinHandle<()>>,
}

impl Worker {
    fn new(id: usize, receiver: Arc<std::sync::Mutex<std::sync::mpsc::Receiver<Job>>>) -> std::thread::JoinHandle<()> {
        std::thread::spawn(move || loop {
            let message = receiver.lock().unwrap().recv();

            match message {
                Ok(job) => {
                    job();
                }
                Err(_) => {
                    break;
                }
            }
        })
    }
}
```

### 5. 索引优化

#### 文件: `src/graph/index/optimized.rs`

```rust
use std::collections::{HashMap, BTreeMap, HashSet};
use crate::core::Value;
use std::sync::{Arc, RwLock};

/// 延迟更新的索引，提高写入性能
pub struct LazyUpdateIndex {
    // 主索引
    primary_index: Arc<RwLock<HashMap<String, HashSet<Value>>>>,
    // 待处理的更新
    pending_updates: Arc<RwLock<Vec<(String, Value, bool)>>>, // (key, value, is_insert)
    // 更新阈值
    update_threshold: usize,
}

impl LazyUpdateIndex {
    pub fn new(update_threshold: usize) -> Self {
        Self {
            primary_index: Arc::new(RwLock::new(HashMap::new())),
            pending_updates: Arc::new(RwLock::new(Vec::new())),
            update_threshold,
        }
    }

    pub fn insert(&self, key: String, value: Value) -> Result<(), String> {
        {
            let mut updates = self.pending_updates.write().unwrap();
            updates.push((key, value, true));
        }

        // 检查是否需要应用更新
        {
            let updates = self.pending_updates.read().unwrap();
            if updates.len() >= self.update_threshold {
                self.apply_pending_updates()?;
            }
        }

        Ok(())
    }

    pub fn remove(&self, key: &str, value: &Value) -> Result<(), String> {
        {
            let mut updates = self.pending_updates.write().unwrap();
            updates.push((key.to_string(), value.clone(), false));
        }

        // 检查是否需要应用更新
        {
            let updates = self.pending_updates.read().unwrap();
            if updates.len() >= self.update_threshold {
                self.apply_pending_updates()?;
            }
        }

        Ok(())
    }

    pub fn get(&self, key: &str) -> Option<HashSet<Value>> {
        // 首先应用待处理的更新
        self.apply_pending_updates().ok()?;

        let index = self.primary_index.read().unwrap();
        index.get(key).cloned()
    }

    fn apply_pending_updates(&self) -> Result<(), String> {
        let mut updates = self.pending_updates.write().unwrap();
        if updates.is_empty() {
            return Ok(());
        }

        let updates_to_process = std::mem::take(&mut *updates);
        drop(updates); // 释放锁以便获取主索引的写锁

        let mut index = self.primary_index.write().unwrap();
        for (key, value, is_insert) in updates_to_process {
            if is_insert {
                index.entry(key).or_insert_with(HashSet::new).insert(value);
            } else {
                if let Some(value_set) = index.get_mut(&key) {
                    value_set.remove(&value);
                    if value_set.is_empty() {
                        index.remove(&key);
                    }
                }
            }
        }

        Ok(())
    }
}

/// 多级索引，支持前缀匹配
pub struct MultiLevelIndex {
    // 一级索引：标签名 -> 节点ID集合
    level1: HashMap<String, HashSet<Value>>,
    // 二级索引：(标签名, 属性名) -> 节点ID集合
    level2: HashMap<(String, String), HashSet<Value>>,
    // 三级索引：(标签名, 属性名, 属性值) -> 节点ID集合
    level3: HashMap<(String, String, Value), HashSet<Value>>,
}

impl MultiLevelIndex {
    pub fn new() -> Self {
        Self {
            level1: HashMap::new(),
            level2: HashMap::new(),
            level3: HashMap::new(),
        }
    }

    pub fn insert(&mut self, tag: String, prop_name: String, prop_value: Value, node_id: Value) {
        // 一级索引
        self.level1.entry(tag.clone()).or_insert_with(HashSet::new).insert(node_id.clone());
        
        // 二级索引
        self.level2.entry((tag.clone(), prop_name.clone())).or_insert_with(HashSet::new).insert(node_id.clone());
        
        // 三级索引
        self.level3.entry((tag, prop_name, prop_value)).or_insert_with(HashSet::new).insert(node_id);
    }

    pub fn query_by_tag(&self, tag: &str) -> Option<&HashSet<Value>> {
        self.level1.get(tag)
    }

    pub fn query_by_tag_and_prop(&self, tag: &str, prop_name: &str) -> Option<&HashSet<Value>> {
        self.level2.get(&(tag.to_string(), prop_name.to_string()))
    }

    pub fn query_by_tag_prop_and_value(&self, tag: &str, prop_name: &str, prop_value: &Value) -> Option<&HashSet<Value>> {
        self.level3.get(&(tag.to_string(), prop_name.to_string(), prop_value.clone()))
    }
}
```

### 6. 统计和监控

#### 文件: `src/stats/performance.rs`

```rust
use std::sync::{Arc, Mutex};
use std::time::{SystemTime, UNIX_EPOCH};
use std::collections::VecDeque;
use std::sync::atomic::{AtomicU64, AtomicUsize, Ordering};

#[derive(Debug)]
pub struct PerformanceStats {
    pub query_count: AtomicU64,
    pub query_time_total: AtomicU64, // 纳秒
    pub storage_reads: AtomicU64,
    pub storage_writes: AtomicU64,
    pub memory_usage: AtomicUsize, // 字节
    pub cache_hits: AtomicU64,
    pub cache_misses: AtomicU64,
    pub active_connections: AtomicUsize,
    pub peak_memory_usage: AtomicUsize,
}

impl PerformanceStats {
    pub fn new() -> Self {
        Self {
            query_count: AtomicU64::new(0),
            query_time_total: AtomicU64::new(0),
            storage_reads: AtomicU64::new(0),
            storage_writes: AtomicU64::new(0),
            memory_usage: AtomicUsize::new(0),
            cache_hits: AtomicU64::new(0),
            cache_misses: AtomicU64::new(0),
            active_connections: AtomicUsize::new(0),
            peak_memory_usage: AtomicUsize::new(0),
        }
    }

    pub fn record_query_time(&self, duration_nanos: u64) {
        self.query_count.fetch_add(1, Ordering::SeqCst);
        self.query_time_total.fetch_add(duration_nanos, Ordering::SeqCst);
    }

    pub fn record_storage_read(&self) {
        self.storage_reads.fetch_add(1, Ordering::SeqCst);
    }

    pub fn record_storage_write(&self) {
        self.storage_writes.fetch_add(1, Ordering::SeqCst);
    }

    pub fn record_cache_hit(&self) {
        self.cache_hits.fetch_add(1, Ordering::SeqCst);
    }

    pub fn record_cache_miss(&self) {
        self.cache_misses.fetch_add(1, Ordering::SeqCst);
    }

    pub fn update_memory_usage(&self, new_usage: usize) {
        self.memory_usage.store(new_usage, Ordering::SeqCst);
        
        // 更新峰值内存使用量
        let mut peak = self.peak_memory_usage.load(Ordering::SeqCst);
        loop {
            if new_usage > peak {
                match self.peak_memory_usage.compare_exchange(peak, new_usage, Ordering::SeqCst, Ordering::SeqCst) {
                    Ok(_) => break,
                    Err(current_peak) => peak = current_peak,
                }
            } else {
                break;
            }
        }
    }

    pub fn avg_query_time_ms(&self) -> f64 {
        let count = self.query_count.load(Ordering::SeqCst);
        if count == 0 {
            0.0
        } else {
            let total_nanos = self.query_time_total.load(Ordering::SeqCst) as f64;
            (total_nanos / count as f64) / 1_000_000.0  // 转换为毫秒
        }
    }

    pub fn cache_hit_rate(&self) -> f64 {
        let hits = self.cache_hits.load(Ordering::SeqCst);
        let misses = self.cache_misses.load(Ordering::SeqCst);
        let total = hits + misses;
        
        if total == 0 {
            0.0
        } else {
            hits as f64 / total as f64
        }
    }
}

pub struct PerformanceMonitor {
    stats: Arc<PerformanceStats>,
    query_times: Arc<Mutex<VecDeque<u64>>>, // 存储最近的查询时间
    max_history_size: usize,
}

impl PerformanceMonitor {
    pub fn new(max_history_size: usize) -> Self {
        Self {
            stats: Arc::new(PerformanceStats::new()),
            query_times: Arc::new(Mutex::new(VecDeque::with_capacity(max_history_size))),
            max_history_size,
        }
    }

    pub fn record_query(&self, duration_nanos: u64) {
        self.stats.record_query_time(duration_nanos);
        
        let mut times = self.query_times.lock().unwrap();
        times.push_back(duration_nanos);
        if times.len() > self.max_history_size {
            times.pop_front();
        }
    }

    pub fn stats(&self) -> &PerformanceStats {
        &self.stats
    }

    pub fn p95_query_time_ms(&self) -> f64 {
        let times = self.query_times.lock().unwrap();
        let mut sorted_times: Vec<u64> = times.iter().cloned().collect();
        sorted_times.sort_unstable();
        
        let n = sorted_times.len();
        if n == 0 {
            0.0
        } else {
            let idx = (n as f64 * 0.95) as usize;
            if idx < n {
                sorted_times[idx] as f64 / 1_000_000.0
            } else {
                sorted_times[n - 1] as f64 / 1_000_000.0
            }
        }
    }
}
```

## 实现建议

1. **分阶段实施**：从查询计划缓存开始，逐步实现其他优化
2. **性能测试**：对每个优化措施进行基准测试
3. **监控集成**：在所有关键路径上集成性能监控
4. **配置化**：让优化参数可配置，以便根据场景调整

## 影响评估

1. **功能增强**：显著提升查询和写入性能
2. **性能**：通过缓存和批量操作提高吞吐量
3. **维护**：引入了更复杂的性能监控和调优选项
4. **兼容性**：所有优化都对现有功能透明，保持向后兼容