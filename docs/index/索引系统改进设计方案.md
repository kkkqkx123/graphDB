# GraphDB索引系统改进设计方案

## 一、概述

本文档基于当前GraphDB索引实现的现状，提出短期、中期、长期的改进设计方案。所有改进均基于单节点架构，不考虑分布式需求。

## 二、短期改进方案

### 2.1 增加前缀查询支持

#### 2.1.1 需求分析

当前索引系统支持精确查询和范围查询，但不支持前缀查询。前缀查询在复合索引场景中非常重要，例如：
- 复合索引 `(name, age)`，查询 `WHERE name = 'Alice'` 只需要匹配前缀
- 字符串索引的前缀匹配，如 `WHERE name LIKE 'A%'`

#### 2.1.2 设计方案

**扩展 IndexBinaryEncoder**

```rust
impl IndexBinaryEncoder {
    /// 编码索引值列表为二进制格式（支持前缀查询）
    pub fn encode_prefix(values: &[Value], prefix_len: usize) -> Vec<u8> {
        let mut buffer = Vec::new();
        let mut var_length_positions = Vec::new();

        for (i, value) in values.iter().enumerate() {
            if i >= prefix_len {
                break;
            }

            match value {
                Value::Int(i) => {
                    buffer.extend_from_slice(&i.to_le_bytes());
                }
                Value::Double(f) => {
                    buffer.extend_from_slice(&f.to_le_bytes());
                }
                Value::Bool(b) => {
                    buffer.push(if *b { 1u8 } else { 0u8 });
                }
                Value::String(s) => {
                    var_length_positions.push(buffer.len());
                    buffer.extend_from_slice(s.as_bytes());
                }
                _ => {
                    panic!("不支持的索引类型: {:?}", value);
                }
            }
        }

        // 在末尾添加可变长度字段的长度信息
        for pos in var_length_positions {
            let length = (buffer.len() - pos) as i32;
            buffer.extend_from_slice(&length.to_le_bytes());
        }

        buffer
    }

    /// 编码前缀范围（用于前缀查询）
    pub fn encode_prefix_range(prefix: &[u8]) -> (Vec<u8>, Vec<u8>) {
        let mut start = prefix.to_vec();
        let mut end = prefix.to_vec();

        // 结束键是前缀加一个最大字节
        end.push(0xFF);

        (start, end)
    }
}
```

**扩展 IndexData 结构**

```rust
impl IndexData {
    /// 前缀查找顶点
    fn prefix_lookup_vertex(&self, prefix: &[u8]) -> Vec<Vertex> {
        let (start, end) = IndexBinaryEncoder::encode_prefix_range(prefix);
        let mut result = Vec::new();
        for (_, vertices) in self.vertex_index.range(start..end) {
            result.extend(vertices.clone());
        }
        result
    }

    /// 前缀查找边
    fn prefix_lookup_edge(&self, prefix: &[u8]) -> Vec<Edge> {
        let (start, end) = IndexBinaryEncoder::encode_prefix_range(prefix);
        let mut result = Vec::new();
        for (_, edges) in self.edge_index.range(start..end) {
            result.extend(edges.clone());
        }
        result
    }
}
```

**扩展 IndexManager trait**

```rust
pub trait IndexManager: Send + Sync + std::fmt::Debug {
    // ... 现有方法 ...

    /// 基于索引的前缀查询顶点
    fn prefix_lookup_vertex(
        &self,
        space_id: i32,
        index_name: &str,
        prefix_values: &[Value],
    ) -> ManagerResult<Vec<Vertex>>;

    /// 基于索引的前缀查询边
    fn prefix_lookup_edge(
        &self,
        space_id: i32,
        index_name: &str,
        prefix_values: &[Value],
    ) -> ManagerResult<Vec<Edge>>;
}
```

#### 2.1.3 实现步骤

1. 在 `index_binary.rs` 中添加 `encode_prefix` 和 `encode_prefix_range` 方法
2. 在 `index_manager_impl.rs` 的 `IndexData` 中添加前缀查询方法
3. 在 `IndexManager` trait 中添加前缀查询接口
4. 在 `MemoryIndexManager` 中实现前缀查询方法

#### 2.1.4 性能优化

- 前缀查询利用 BTreeMap 的范围查询能力，时间复杂度为 O(log n + k)
- 前缀编码可以缓存，避免重复计算
- 支持前缀长度参数，灵活控制查询范围

### 2.2 优化并发控制

#### 2.2.1 需求分析

当前使用 `Arc<RwLock<HashMap>>` 进行并发控制，存在以下问题：
- 粗粒度锁：所有索引数据共享一个读写锁
- 写操作阻塞：写操作会阻塞所有读操作
- 扩展性差：并发写入时性能下降明显

#### 2.2.2 设计方案

**方案一：使用 DashMap 替代 RwLock**

```rust
use dashmap::DashMap;

struct IndexData {
    vertex_index: DashMap<Vec<u8>, Vec<Vertex>>,
    edge_index: DashMap<Vec<u8>, Vec<Edge>>,
}

impl IndexData {
    fn new() -> Self {
        Self {
            vertex_index: DashMap::new(),
            edge_index: DashMap::new(),
        }
    }

    fn insert_vertex(&self, key: Vec<u8>, vertex: Vertex) {
        self.vertex_index.entry(key).or_insert_with(Vec::new).push(vertex);
    }

    fn lookup_vertex(&self, key: &[u8]) -> Option<Vec<Vertex>> {
        self.vertex_index.get(key).map(|v| v.clone())
    }

    fn range_lookup_vertex(&self, start: &[u8], end: &[u8]) -> Vec<Vertex> {
        let mut result = Vec::new();
        self.vertex_index.range(start.to_vec()..=end.to_vec())
            .for_each(|entry| result.extend(entry.value().clone()));
        result
    }
}
```

**方案二：分片锁（Sharded Lock）**

```rust
use std::sync::{Arc, RwLock};
use std::collections::HashMap;

const SHARD_COUNT: usize = 16;

struct ShardedIndexData {
    shards: [Arc<RwLock<HashMap<Vec<u8>, Vec<Vertex>>>>; SHARD_COUNT],
}

impl ShardedIndexData {
    fn new() -> Self {
        const INIT: Arc<RwLock<HashMap<Vec<u8>, Vec<Vertex>>>> =
            Arc::new(RwLock::new(HashMap::new()));
        Self {
            shards: [INIT; SHARD_COUNT],
        }
    }

    fn get_shard(&self, key: &[u8]) -> &Arc<RwLock<HashMap<Vec<u8>, Vec<Vertex>>>> {
        let hash = std::collections::hash_map::DefaultHasher::new();
        let mut hasher = hash;
        use std::hash::Hash;
        key.hash(&mut hasher);
        &self.shards[(hasher.finish() as usize) % SHARD_COUNT]
    }

    fn insert_vertex(&self, key: Vec<u8>, vertex: Vertex) {
        let shard = self.get_shard(&key);
        let mut map = shard.write().unwrap();
        map.entry(key).or_insert_with(Vec::new).push(vertex);
    }

    fn lookup_vertex(&self, key: &[u8]) -> Option<Vec<Vertex>> {
        let shard = self.get_shard(key);
        let map = shard.read().unwrap();
        map.get(key).cloned()
    }
}
```

**推荐方案：DashMap**

- DashMap 是专门为并发场景优化的哈希表
- 提供细粒度的锁，支持高并发读写
- API 简洁，易于使用
- 性能优秀，适合高并发场景

#### 2.2.3 实现步骤

1. 在 `Cargo.toml` 中添加 `dashmap` 依赖
2. 修改 `IndexData` 结构，使用 `DashMap` 替代 `BTreeMap`
3. 更新 `IndexData` 的所有方法，使用 DashMap 的 API
4. 移除 `Arc<RwLock<>>` 包装，因为 DashMap 本身是并发安全的
5. 测试并发性能，验证改进效果

#### 2.2.4 性能优化

- DashMap 的读操作无锁，写操作只锁定相关分片
- 支持并发读写，读操作不会阻塞写操作
- 自动负载均衡，避免热点问题

### 2.3 完善持久化

#### 2.3.1 需求分析

当前持久化机制存在以下问题：
- 只持久化索引元数据（JSON 文件）
- 不持久化索引数据
- 进程重启后需要重新构建索引
- 构建时间随数据量增长而增长

#### 2.3.2 设计方案

**方案一：增量持久化**

```rust
use std::fs::File;
use std::io::{BufWriter, Write};
use std::path::PathBuf;

struct IndexPersistence {
    storage_path: PathBuf,
}

impl IndexPersistence {
    /// 持久化索引数据
    fn persist_index_data(&self, index_id: i32, index_data: &IndexData) -> Result<(), String> {
        let index_dir = self.storage_path.join(format!("index_{}", index_id));
        std::fs::create_dir_all(&index_dir)
            .map_err(|e| format!("创建索引目录失败: {}", e))?;

        // 持久化顶点索引
        let vertex_file = index_dir.join("vertex_index.bin");
        self.persist_vertex_index(&vertex_file, &index_data.vertex_index)?;

        // 持久化边索引
        let edge_file = index_dir.join("edge_index.bin");
        self.persist_edge_index(&edge_file, &index_data.edge_index)?;

        Ok(())
    }

    /// 持久化顶点索引
    fn persist_vertex_index(
        &self,
        file_path: &PathBuf,
        index: &BTreeMap<Vec<u8>, Vec<Vertex>>,
    ) -> Result<(), String> {
        let file = File::create(file_path)
            .map_err(|e| format!("创建文件失败: {}", e))?;
        let mut writer = BufWriter::new(file);

        // 写入条目数量
        let count = index.len() as u64;
        writer.write_all(&count.to_le_bytes())
            .map_err(|e| format!("写入数量失败: {}", e))?;

        // 写入每个条目
        for (key, vertices) in index {
            // 写入键长度
            let key_len = key.len() as u32;
            writer.write_all(&key_len.to_le_bytes())
                .map_err(|e| format!("写入键长度失败: {}", e))?;
            // 写入键
            writer.write_all(key)
                .map_err(|e| format!("写入键失败: {}", e))?;
            // 写入顶点数量
            let vertex_count = vertices.len() as u32;
            writer.write_all(&vertex_count.to_le_bytes())
                .map_err(|e| format!("写入顶点数量失败: {}", e))?;
            // 写入顶点
            for vertex in vertices {
                let vertex_bytes = bincode::serialize(vertex)
                    .map_err(|e| format!("序列化顶点失败: {}", e))?;
                let vertex_len = vertex_bytes.len() as u32;
                writer.write_all(&vertex_len.to_le_bytes())
                    .map_err(|e| format!("写入顶点长度失败: {}", e))?;
                writer.write_all(&vertex_bytes)
                    .map_err(|e| format!("写入顶点失败: {}", e))?;
            }
        }

        writer.flush()
            .map_err(|e| format!("刷新缓冲区失败: {}", e))?;

        Ok(())
    }

    /// 加载索引数据
    fn load_index_data(&self, index_id: i32) -> Result<IndexData, String> {
        let index_dir = self.storage_path.join(format!("index_{}", index_id));
        let vertex_file = index_dir.join("vertex_index.bin");
        let edge_file = index_dir.join("edge_index.bin");

        let vertex_index = self.load_vertex_index(&vertex_file)?;
        let edge_index = self.load_edge_index(&edge_file)?;

        Ok(IndexData {
            vertex_index,
            edge_index,
        })
    }
}
```

**方案二：定期快照 + WAL**

```rust
use std::sync::Arc;
use std::sync::atomic::{AtomicBool, Ordering};

struct IndexSnapshotManager {
    storage_path: PathBuf,
    is_snapshotting: Arc<AtomicBool>,
}

impl IndexSnapshotManager {
    /// 创建快照
    fn create_snapshot(&self, index_id: i32, index_data: &IndexData) -> Result<(), String> {
        if self.is_snapshotting.load(Ordering::Relaxed) {
            return Err("快照正在进行中".to_string());
        }

        self.is_snapshotting.store(true, Ordering::Relaxed);

        let result = self.do_create_snapshot(index_id, index_data);

        self.is_snapshotting.store(false, Ordering::Relaxed);

        result
    }

    fn do_create_snapshot(&self, index_id: i32, index_data: &IndexData) -> Result<(), String> {
        let snapshot_dir = self.storage_path.join(format!("snapshot_{}", index_id));
        std::fs::create_dir_all(&snapshot_dir)
            .map_err(|e| format!("创建快照目录失败: {}", e))?;

        // 写入快照文件
        let snapshot_file = snapshot_dir.join(format!("snapshot_{}.bin", chrono::Utc::now().timestamp()));
        // ... 持久化逻辑 ...

        Ok(())
    }
}

struct IndexWAL {
    wal_file: File,
}

impl IndexWAL {
    /// 追加索引变更
    fn append_change(&mut self, index_id: i32, change: IndexChange) -> Result<(), String> {
        let change_bytes = bincode::serialize(&change)
            .map_err(|e| format!("序列化变更失败: {}", e))?;
        let change_len = change_bytes.len() as u32;

        self.wal_file.write_all(&change_len.to_le_bytes())
            .map_err(|e| format!("写入变更长度失败: {}", e))?;
        self.wal_file.write_all(&change_bytes)
            .map_err(|e| format!("写入变更失败: {}", e))?;
        self.wal_file.flush()
            .map_err(|e| format!("刷新WAL失败: {}", e))?;

        Ok(())
    }
}

#[derive(Serialize, Deserialize)]
enum IndexChange {
    InsertVertex { index_id: i32, key: Vec<u8>, vertex: Vertex },
    DeleteVertex { index_id: i32, key: Vec<u8>, vertex_id: u64 },
    InsertEdge { index_id: i32, key: Vec<u8>, edge: Edge },
    DeleteEdge { index_id: i32, key: Vec<u8>, edge_id: u64 },
}
```

**推荐方案：增量持久化 + 定期快照**

- 增量持久化：每次索引变更后追加到 WAL
- 定期快照：每隔一段时间创建完整快照
- 恢复机制：先加载最新快照，再回放 WAL

#### 2.3.3 实现步骤

1. 在 `Cargo.toml` 中添加 `bincode` 和 `chrono` 依赖
2. 创建 `IndexPersistence` 模块，实现索引数据的持久化和加载
3. 在 `MemoryIndexManager` 中集成持久化逻辑
4. 在索引变更操作（插入、删除、更新）后触发持久化
5. 在 `load_from_disk` 中加载索引数据
6. 实现定期快照机制

#### 2.3.4 性能优化

- 异步持久化：使用后台线程进行持久化，不阻塞主流程
- 批量持久化：累积一定数量的变更后批量写入
- 压缩存储：使用压缩算法减少存储空间
- 增量加载：支持增量加载，减少启动时间

## 三、中期改进方案

### 3.1 引入缓存机制

#### 3.1.1 需求分析

当前索引数据全部存储在内存中，存在以下问题：
- 内存占用随数据量增长而增长
- 冷数据占用大量内存
- 内存受限时无法处理大规模数据

#### 3.1.2 设计方案

**LRU 缓存 + 磁盘存储**

```rust
use lru::LruCache;
use std::num::NonZeroUsize;

struct CachedIndexData {
    cache: Arc<RwLock<LruCache<Vec<u8>, Vec<Vertex>>>>,
    disk_storage: Arc<DiskIndexStorage>,
    max_cache_size: usize,
}

impl CachedIndexData {
    fn new(max_cache_size: usize, disk_path: PathBuf) -> Self {
        Self {
            cache: Arc::new(RwLock::new(LruCache::new(
                NonZeroUsize::new(max_cache_size).unwrap()
            ))),
            disk_storage: Arc::new(DiskIndexStorage::new(disk_path)),
            max_cache_size,
        }
    }

    fn lookup_vertex(&self, key: &[u8]) -> Option<Vec<Vertex>> {
        // 先查缓存
        {
            let mut cache = self.cache.write().unwrap();
            if let Some(vertices) = cache.get(key) {
                return Some(vertices.clone());
            }
        }

        // 缓存未命中，从磁盘加载
        if let Some(vertices) = self.disk_storage.load_vertex(key) {
            // 放入缓存
            let mut cache = self.cache.write().unwrap();
            cache.put(key.to_vec(), vertices.clone());
            Some(vertices)
        } else {
            None
        }
    }

    fn insert_vertex(&self, key: Vec<u8>, vertex: Vertex) {
        // 更新缓存
        let mut cache = self.cache.write().unwrap();
        cache.entry(key.clone()).or_insert_with(Vec::new).push(vertex.clone());
        drop(cache);

        // 持久化到磁盘
        self.disk_storage.store_vertex(key, vertex);
    }
}

struct DiskIndexStorage {
    storage_path: PathBuf,
}

impl DiskIndexStorage {
    fn new(storage_path: PathBuf) -> Self {
        Self { storage_path }
    }

    fn store_vertex(&self, key: Vec<u8>, vertex: Vertex) {
        // 实现磁盘存储逻辑
    }

    fn load_vertex(&self, key: &[u8]) -> Option<Vec<Vertex>> {
        // 实现磁盘加载逻辑
        None
    }
}
```

**两级缓存（热数据 + 温数据）**

```rust
struct TwoLevelCache {
    hot_cache: Arc<RwLock<LruCache<Vec<u8>, Vec<Vertex>>>>,
    warm_cache: Arc<RwLock<LruCache<Vec<u8>, Vec<Vertex>>>>,
    disk_storage: Arc<DiskIndexStorage>,
}

impl TwoLevelCache {
    fn lookup_vertex(&self, key: &[u8]) -> Option<Vec<Vertex>> {
        // 查热缓存
        {
            let mut hot = self.hot_cache.write().unwrap();
            if let Some(vertices) = hot.get(key) {
                return Some(vertices.clone());
            }
        }

        // 查温缓存
        {
            let mut warm = self.warm_cache.write().unwrap();
            if let Some(vertices) = warm.get(key) {
                // 提升到热缓存
                let mut hot = self.hot_cache.write().unwrap();
                hot.put(key.to_vec(), vertices.clone());
                return Some(vertices);
            }
        }

        // 从磁盘加载
        if let Some(vertices) = self.disk_storage.load_vertex(key) {
            // 放入温缓存
            let mut warm = self.warm_cache.write().unwrap();
            warm.put(key.to_vec(), vertices.clone());
            Some(vertices)
        } else {
            None
        }
    }
}
```

**推荐方案：LRU 缓存 + 磁盘存储**

- 实现简单，易于维护
- LRU 策略自动淘汰冷数据
- 支持动态调整缓存大小
- 磁盘存储提供无限容量

#### 3.1.3 实现步骤

1. 在 `Cargo.toml` 中添加 `lru` 依赖
2. 创建 `cache` 模块，实现缓存机制
3. 修改 `IndexData` 结构，集成缓存
4. 实现磁盘存储层
5. 添加缓存统计和监控

#### 3.1.4 性能优化

- 预取机制：预取可能访问的数据
- 批量加载：批量加载相关数据，减少 I/O
- 压缩存储：磁盘数据压缩存储
- 异步加载：后台异步加载冷数据

### 3.2 支持复合索引优化

#### 3.2.1 需求分析

当前索引系统支持复合索引，但缺乏优化策略：
- 查询优化器无法选择最优索引
- 复合索引的前缀匹配未充分利用
- 索引选择策略简单粗暴

#### 3.2.2 设计方案

**索引选择器**

```rust
struct IndexSelector {
    indexes: Arc<RwLock<HashMap<String, Index>>>,
}

impl IndexSelector {
    /// 为查询选择最优索引
    fn select_index(&self, query: &Query) -> Option<SelectedIndex> {
        let indexes = self.indexes.read().unwrap();
        let candidate_indexes: Vec<&Index> = indexes.values()
            .filter(|idx| self.is_index_relevant(idx, query))
            .collect();

        if candidate_indexes.is_empty() {
            return None;
        }

        // 评估每个索引的得分
        let mut scored_indexes: Vec<(f64, &Index)> = candidate_indexes
            .iter()
            .map(|idx| (self.score_index(idx, query), *idx))
            .collect();

        // 按得分排序
        scored_indexes.sort_by(|a, b| b.0.partial_cmp(&a.0).unwrap());

        // 返回得分最高的索引
        scored_indexes.first().map(|(_, idx)| SelectedIndex {
            index: idx.clone(),
            match_type: self.determine_match_type(idx, query),
        })
    }

    /// 评估索引得分
    fn score_index(&self, index: &Index, query: &Query) -> f64 {
        let mut score = 0.0;

        // 前缀匹配得分
        let prefix_match_count = self.count_prefix_matches(index, query);
        score += prefix_match_count as f64 * 10.0;

        // 唯一性得分
        if index.is_unique {
            score += 5.0;
        }

        // 选择性得分
        let selectivity = self.estimate_selectivity(index);
        score += selectivity * 20.0;

        score
    }

    /// 估计索引选择性
    fn estimate_selectivity(&self, index: &Index) -> f64 {
        // 使用统计信息估计选择性
        // 选择性 = distinct_count / total_count
        // 越接近 1.0，选择性越好
        0.5 // 默认值
    }
}

struct SelectedIndex {
    index: Index,
    match_type: IndexMatchType,
}

enum IndexMatchType {
    FullMatch,
    PrefixMatch,
    PartialMatch,
}
```

**复合索引统计信息**

```rust
#[derive(Debug, Clone, Serialize, Deserialize)]
struct IndexStatistics {
    index_id: i32,
    total_count: u64,
    distinct_count: u64,
    null_count: u64,
    field_statistics: Vec<FieldStatistics>,
    last_updated: i64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct FieldStatistics {
    field_name: String,
    distinct_count: u64,
    null_count: u64,
    min_value: Option<Value>,
    max_value: Option<Value>,
    histogram: Vec<HistogramBucket>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct HistogramBucket {
    lower_bound: Value,
    upper_bound: Value,
    count: u64,
}

struct IndexStatisticsCollector {
    statistics: Arc<RwLock<HashMap<i32, IndexStatistics>>>,
}

impl IndexStatisticsCollector {
    /// 收集索引统计信息
    fn collect_statistics(&self, index_id: i32, index_data: &IndexData) {
        let stats = IndexStatistics {
            index_id,
            total_count: self.count_total(index_data),
            distinct_count: self.count_distinct(index_data),
            null_count: self.count_nulls(index_data),
            field_statistics: vec![],
            last_updated: chrono::Utc::now().timestamp(),
        };

        let mut statistics = self.statistics.write().unwrap();
        statistics.insert(index_id, stats);
    }

    fn count_total(&self, index_data: &IndexData) -> u64 {
        index_data.vertex_index.len() as u64 + index_data.edge_index.len() as u64
    }

    fn count_distinct(&self, index_data: &IndexData) -> u64 {
        index_data.vertex_index.len() as u64
    }

    fn count_nulls(&self, index_data: &IndexData) -> u64 {
        0 // 实现 null 统计
    }
}
```

**推荐方案：索引选择器 + 统计信息**

- 索引选择器根据查询特征选择最优索引
- 统计信息帮助评估索引质量
- 支持多种匹配类型（完全匹配、前缀匹配、部分匹配）

#### 3.2.3 实现步骤

1. 创建 `index_selector` 模块，实现索引选择逻辑
2. 创建 `index_statistics` 模块，实现统计信息收集
3. 在 `IndexManager` 中集成索引选择器
4. 在索引构建后收集统计信息
5. 在查询优化器中使用索引选择器

#### 3.2.4 性能优化

- 缓存选择结果：避免重复计算
- 增量更新统计：统计信息增量更新
- 自适应选择：根据实际查询结果调整选择策略

### 3.3 添加索引统计信息

#### 3.3.1 需求分析

统计信息对于查询优化和索引选择至关重要：
- 帮助查询优化器选择最优执行计划
- 评估索引的选择性和质量
- 监控索引使用情况

#### 3.3.2 设计方案

**统计信息收集器**

```rust
struct IndexStatisticsManager {
    statistics: Arc<RwLock<HashMap<i32, IndexStatistics>>>,
    storage_path: PathBuf,
}

impl IndexStatisticsManager {
    /// 收集索引统计信息
    fn collect(&self, index_id: i32, index_data: &IndexData) -> IndexStatistics {
        let vertex_stats = self.collect_vertex_statistics(&index_data.vertex_index);
        let edge_stats = self.collect_edge_statistics(&index_data.edge_index);

        IndexStatistics {
            index_id,
            total_count: vertex_stats.total_count + edge_stats.total_count,
            distinct_count: vertex_stats.distinct_count + edge_stats.distinct_count,
            null_count: vertex_stats.null_count + edge_stats.null_count,
            field_statistics: vec![],
            last_updated: chrono::Utc::now().timestamp(),
        }
    }

    /// 收集顶点索引统计
    fn collect_vertex_statistics(&self, index: &BTreeMap<Vec<u8>, Vec<Vertex>>) -> IndexStatistics {
        let total_count = index.len() as u64;
        let distinct_count = total_count;
        let null_count = 0;

        // 收集字段统计
        let mut field_stats: HashMap<String, FieldStatistics> = HashMap::new();

        for (_, vertices) in index {
            for vertex in vertices {
                for tag in &vertex.tags {
                    for (field_name, value) in &tag.properties {
                        let stats = field_stats.entry(field_name.clone()).or_insert_with(|| {
                            FieldStatistics {
                                field_name: field_name.clone(),
                                distinct_count: 0,
                                null_count: 0,
                                min_value: None,
                                max_value: None,
                                histogram: vec![],
                            }
                        });

                        // 更新统计信息
                        stats.distinct_count += 1;
                        self.update_min_max(stats, value);
                    }
                }
            }
        }

        IndexStatistics {
            index_id: 0,
            total_count,
            distinct_count,
            null_count,
            field_statistics: field_stats.values().cloned().collect(),
            last_updated: chrono::Utc::now().timestamp(),
        }
    }

    fn update_min_max(&self, stats: &mut FieldStatistics, value: &Value) {
        match value {
            Value::Int(i) => {
                if stats.min_value.is_none() {
                    stats.min_value = Some(value.clone());
                    stats.max_value = Some(value.clone());
                } else {
                    if let Some(Value::Int(min)) = &stats.min_value {
                        if i < min {
                            stats.min_value = Some(value.clone());
                        }
                    }
                    if let Some(Value::Int(max)) = &stats.max_value {
                        if i > max {
                            stats.max_value = Some(value.clone());
                        }
                    }
                }
            }
            // 其他类型...
            _ => {}
        }
    }
}
```

**统计信息持久化**

```rust
impl IndexStatisticsManager {
    /// 保存统计信息到磁盘
    fn save_statistics(&self, index_id: i32, stats: &IndexStatistics) -> Result<(), String> {
        let stats_file = self.storage_path.join(format!("stats_{}.json", index_id));
        let content = serde_json::to_string_pretty(stats)
            .map_err(|e| format!("序列化统计信息失败: {}", e))?;
        std::fs::write(&stats_file, content)
            .map_err(|e| format!("写入统计信息失败: {}", e))?;
        Ok(())
    }

    /// 从磁盘加载统计信息
    fn load_statistics(&self, index_id: i32) -> Result<Option<IndexStatistics>, String> {
        let stats_file = self.storage_path.join(format!("stats_{}.json", index_id));
        if !stats_file.exists() {
            return Ok(None);
        }

        let content = std::fs::read_to_string(&stats_file)
            .map_err(|e| format!("读取统计信息失败: {}", e))?;
        let stats: IndexStatistics = serde_json::from_str(&content)
            .map_err(|e| format!("反序列化统计信息失败: {}", e))?;
        Ok(Some(stats))
    }
}
```

**推荐方案：完整统计信息 + 持久化**

- 收集全面的统计信息（总数、唯一数、空值、直方图等）
- 支持统计信息的持久化和加载
- 定期更新统计信息，保持准确性

#### 3.3.3 实现步骤

1. 创建 `index_statistics` 模块
2. 定义统计信息数据结构
3. 实现统计信息收集逻辑
4. 实现统计信息持久化
5. 在索引构建后收集统计信息
6. 提供统计信息查询接口

#### 3.3.4 性能优化

- 采样统计：对大规模数据使用采样统计
- 增量更新：统计信息增量更新，避免全量重算
- 异步收集：后台异步收集统计信息

## 四、长期改进方案

### 4.1 支持磁盘存储

#### 4.1.1 需求分析

当前索引数据全部存储在内存中，限制了数据规模：
- 内存受限，无法处理大规模数据
- 进程重启后需要重新构建索引
- 内存成本高

#### 4.1.2 设计方案

**基于 RocksDB 的磁盘存储**

```rust
use rocksdb::{DB, Options, WriteBatch};

struct DiskIndexStorage {
    db: Arc<DB>,
}

impl DiskIndexStorage {
    /// 创建磁盘索引存储
    fn new(db_path: &str) -> Result<Self, String> {
        let mut opts = Options::default();
        opts.create_if_missing(true);
        opts.set_compression_type(rocksdb::DBCompressionType::Lz4);

        let db = DB::open(&opts, db_path)
            .map_err(|e| format!("打开数据库失败: {}", e))?;

        Ok(Self {
            db: Arc::new(db),
        })
    }

    /// 存储顶点索引
    fn store_vertex_index(&self, index_id: i32, key: &[u8], vertices: &[Vertex]) -> Result<(), String> {
        let key = self.encode_vertex_key(index_id, key);
        let value = bincode::serialize(vertices)
            .map_err(|e| format!("序列化顶点失败: {}", e))?;

        self.db.put(&key, &value)
            .map_err(|e| format!("写入顶点索引失败: {}", e))?;

        Ok(())
    }

    /// 加载顶点索引
    fn load_vertex_index(&self, index_id: i32, key: &[u8]) -> Result<Option<Vec<Vertex>>, String> {
        let key = self.encode_vertex_key(index_id, key);

        match self.db.get(&key) {
            Ok(Some(value)) => {
                let vertices: Vec<Vertex> = bincode::deserialize(&value)
                    .map_err(|e| format!("反序列化顶点失败: {}", e))?;
                Ok(Some(vertices))
            }
            Ok(None) => Ok(None),
            Err(e) => Err(format!("读取顶点索引失败: {}", e)),
        }
    }

    /// 批量存储顶点索引
    fn batch_store_vertex_index(&self, entries: Vec<(i32, Vec<u8>, Vec<Vertex>)>) -> Result<(), String> {
        let mut batch = WriteBatch::default();

        for (index_id, key, vertices) in entries {
            let key = self.encode_vertex_key(index_id, &key);
            let value = bincode::serialize(&vertices)
                .map_err(|e| format!("序列化顶点失败: {}", e))?;
            batch.put(&key, &value);
        }

        self.db.write(batch)
            .map_err(|e| format!("批量写入失败: {}", e))?;

        Ok(())
    }

    /// 范围查询顶点索引
    fn range_scan_vertex_index(
        &self,
        index_id: i32,
        start: &[u8],
        end: &[u8],
    ) -> Result<Vec<Vertex>, String> {
        let start_key = self.encode_vertex_key(index_id, start);
        let end_key = self.encode_vertex_key(index_id, end);

        let mut result = Vec::new();
        let iter = self.db.iterator(rocksdb::IteratorMode::From(
            &start_key,
            rocksdb::Direction::Forward,
        ));

        for item in iter {
            let (key, value) = item
                .map_err(|e| format!("迭代失败: {}", e))?;

            if key > end_key.as_slice() {
                break;
            }

            let vertices: Vec<Vertex> = bincode::deserialize(&value)
                .map_err(|e| format!("反序列化顶点失败: {}", e))?;
            result.extend(vertices);
        }

        Ok(result)
    }

    fn encode_vertex_key(&self, index_id: i32, key: &[u8]) -> Vec<u8> {
        let mut encoded = Vec::new();
        encoded.extend_from_slice(&index_id.to_le_bytes());
        encoded.extend_from_slice(b"v:");
        encoded.extend_from_slice(key);
        encoded
    }

    fn encode_edge_key(&self, index_id: i32, key: &[u8]) -> Vec<u8> {
        let mut encoded = Vec::new();
        encoded.extend_from_slice(&index_id.to_le_bytes());
        encoded.extend_from_slice(b"e:");
        encoded.extend_from_slice(key);
        encoded
    }
}
```

**混合存储（内存 + 磁盘）**

```rust
struct HybridIndexStorage {
    memory_cache: Arc<RwLock<LruCache<Vec<u8>, Vec<Vertex>>>>,
    disk_storage: Arc<DiskIndexStorage>,
    cache_size: usize,
}

impl HybridIndexStorage {
    fn new(db_path: &str, cache_size: usize) -> Result<Self, String> {
        Ok(Self {
            memory_cache: Arc::new(RwLock::new(LruCache::new(
                NonZeroUsize::new(cache_size).unwrap()
            ))),
            disk_storage: Arc::new(DiskIndexStorage::new(db_path)?),
            cache_size,
        })
    }

    fn lookup_vertex(&self, index_id: i32, key: &[u8]) -> Result<Option<Vec<Vertex>>, String> {
        // 先查内存缓存
        {
            let mut cache = self.memory_cache.write().unwrap();
            if let Some(vertices) = cache.get(key) {
                return Ok(Some(vertices.clone()));
            }
        }

        // 缓存未命中，从磁盘加载
        match self.disk_storage.load_vertex_index(index_id, key)? {
            Some(vertices) => {
                // 放入缓存
                let mut cache = self.memory_cache.write().unwrap();
                cache.put(key.to_vec(), vertices.clone());
                Ok(Some(vertices))
            }
            None => Ok(None),
        }
    }

    fn insert_vertex(&self, index_id: i32, key: Vec<u8>, vertices: Vec<Vertex>) -> Result<(), String> {
        // 更新缓存
        {
            let mut cache = self.memory_cache.write().unwrap();
            cache.put(key.clone(), vertices.clone());
        }

        // 持久化到磁盘
        self.disk_storage.store_vertex_index(index_id, &key, &vertices)?;

        Ok(())
    }
}
```

**推荐方案：混合存储（内存缓存 + RocksDB 磁盘存储）**

- 内存缓存提供快速访问
- RocksDB 提供持久化和大容量存储
- LRU 缓存自动管理内存使用
- 支持批量操作，提高写入性能

#### 4.1.3 实现步骤

1. 在 `Cargo.toml` 中添加 `rocksdb` 依赖
2. 创建 `disk_storage` 模块，实现 RocksDB 存储层
3. 创建 `hybrid_storage` 模块，实现混合存储
4. 修改 `IndexData` 结构，使用混合存储
5. 实现索引数据的迁移（从内存到磁盘）
6. 添加存储配置选项（缓存大小、压缩算法等）

#### 4.1.4 性能优化

- 批量写入：使用 WriteBatch 批量写入
- 压缩存储：使用 LZ4 压缩减少存储空间
- 前缀压缩：RocksDB 的前缀压缩减少键存储
- Bloom Filter：使用 Bloom Filter 减少磁盘读取
- 并发压缩：后台异步压缩数据

### 4.2 完善事务机制

#### 4.2.1 需求分析

当前索引更新缺乏事务保证：
- 索引更新与数据更新不在同一事务中
- 可能出现索引与数据不一致
- 并发更新时可能出现冲突

#### 4.2.2 设计方案

**事务管理器**

```rust
use std::sync::Arc;
use std::sync::atomic::{AtomicU64, Ordering};

struct TransactionManager {
    next_tx_id: Arc<AtomicU64>,
    active_transactions: Arc<RwLock<HashMap<u64, Transaction>>>,
    index_manager: Arc<MemoryIndexManager>,
}

impl TransactionManager {
    /// 开始事务
    fn begin_transaction(&self) -> u64 {
        let tx_id = self.next_tx_id.fetch_add(1, Ordering::Relaxed);
        let tx = Transaction::new(tx_id);

        let mut active = self.active_transactions.write().unwrap();
        active.insert(tx_id, tx);

        tx_id
    }

    /// 提交事务
    fn commit_transaction(&self, tx_id: u64) -> Result<(), String> {
        let mut active = self.active_transactions.write().unwrap();
        let tx = active.remove(&tx_id)
            .ok_or_else(|| format!("事务 {} 不存在", tx_id))?;

        // 应用所有变更
        for change in tx.changes {
            self.apply_change(change)?;
        }

        Ok(())
    }

    /// 回滚事务
    fn rollback_transaction(&self, tx_id: u64) -> Result<(), String> {
        let mut active = self.active_transactions.write().unwrap();
        active.remove(&tx_id)
            .ok_or_else(|| format!("事务 {} 不存在", tx_id))?;
        Ok(())
    }

    fn apply_change(&self, change: TransactionChange) -> Result<(), String> {
        match change {
            TransactionChange::InsertVertex { space_id, vertex } => {
                self.index_manager.insert_vertex_to_index(space_id, &vertex)?;
            }
            TransactionChange::DeleteVertex { space_id, vertex } => {
                self.index_manager.delete_vertex_from_index(space_id, &vertex)?;
            }
            TransactionChange::UpdateVertex { space_id, old_vertex, new_vertex } => {
                self.index_manager.update_vertex_in_index(space_id, &old_vertex, &new_vertex)?;
            }
            // 其他变更...
        }
        Ok(())
    }
}

struct Transaction {
    id: u64,
    start_time: i64,
    changes: Vec<TransactionChange>,
    isolation_level: IsolationLevel,
}

impl Transaction {
    fn new(id: u64) -> Self {
        Self {
            id,
            start_time: chrono::Utc::now().timestamp(),
            changes: Vec::new(),
            isolation_level: IsolationLevel::ReadCommitted,
        }
    }

    fn add_change(&mut self, change: TransactionChange) {
        self.changes.push(change);
    }
}

#[derive(Debug, Clone)]
enum TransactionChange {
    InsertVertex { space_id: i32, vertex: Vertex },
    DeleteVertex { space_id: i32, vertex: Vertex },
    UpdateVertex { space_id: i32, old_vertex: Vertex, new_vertex: Vertex },
    InsertEdge { space_id: i32, edge: Edge },
    DeleteEdge { space_id: i32, edge: Edge },
    UpdateEdge { space_id: i32, old_edge: Edge, new_edge: Edge },
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
enum IsolationLevel {
    ReadUncommitted,
    ReadCommitted,
    RepeatableRead,
    Serializable,
}
```

**MVCC（多版本并发控制）**

```rust
struct MVCCIndexData {
    versions: Arc<RwLock<HashMap<Vec<u8>, Vec<VersionedEntry>>>>,
}

#[derive(Debug, Clone)]
struct VersionedEntry {
    vertex: Vertex,
    tx_id: u64,
    timestamp: i64,
    is_deleted: bool,
}

impl MVCCIndexData {
    fn lookup_vertex(&self, key: &[u8], tx_id: u64, isolation_level: IsolationLevel) -> Option<Vertex> {
        let versions = self.versions.read().unwrap();
        let entries = versions.get(key)?;

        match isolation_level {
            IsolationLevel::ReadCommitted => {
                // 返回已提交的最新版本
                entries.iter()
                    .filter(|e| !e.is_deleted)
                    .filter(|e| self.is_committed(e.tx_id))
                    .max_by_key(|e| e.timestamp)
                    .map(|e| e.vertex.clone())
            }
            IsolationLevel::RepeatableRead => {
                // 返回事务开始时的版本
                entries.iter()
                    .filter(|e| !e.is_deleted)
                    .filter(|e| self.is_visible(e, tx_id))
                    .max_by_key(|e| e.timestamp)
                    .map(|e| e.vertex.clone())
            }
            _ => None,
        }
    }

    fn is_committed(&self, tx_id: u64) -> bool {
        // 检查事务是否已提交
        true
    }

    fn is_visible(&self, entry: &VersionedEntry, tx_id: u64) -> bool {
        // 检查版本是否对事务可见
        entry.tx_id <= tx_id
    }

    fn insert_vertex(&self, key: Vec<u8>, vertex: Vertex, tx_id: u64) {
        let mut versions = self.versions.write().unwrap();
        let entry = VersionedEntry {
            vertex,
            tx_id,
            timestamp: chrono::Utc::now().timestamp(),
            is_deleted: false,
        };
        versions.entry(key).or_insert_with(Vec::new).push(entry);
    }

    fn delete_vertex(&self, key: &[u8], tx_id: u64) {
        let mut versions = self.versions.write().unwrap();
        if let Some(entries) = versions.get_mut(key) {
            if let Some(last) = entries.last_mut() {
                last.is_deleted = true;
            }
        }
    }
}
```

**推荐方案：事务管理器 + MVCC**

- 事务管理器提供完整的事务支持
- MVCC 实现多版本并发控制
- 支持多种隔离级别
- 保证索引与数据的一致性

#### 4.2.3 实现步骤

1. 创建 `transaction` 模块
2. 定义事务相关数据结构
3. 实现事务管理器
4. 实现 MVCC 机制
5. 在索引操作中集成事务
6. 实现事务日志和恢复

#### 4.2.4 性能优化

- 乐观并发控制：减少锁竞争
- 延迟写入：批量提交变更
- 版本清理：定期清理旧版本
- 读写分离：读操作不加锁

## 五、实施计划

### 5.1 短期实施（1-2个月）

**优先级：高**

1. **前缀查询支持**（2周）
   - 第1周：设计并实现前缀编码
   - 第2周：集成到索引管理器，编写测试

2. **并发控制优化**（2周）
   - 第1周：集成 DashMap，替换 RwLock
   - 第2周：性能测试和优化

3. **持久化完善**（3周）
   - 第1周：设计持久化方案
   - 第2周：实现增量持久化
   - 第3周：实现定期快照和恢复

### 5.2 中期实施（3-4个月）

**优先级：中**

1. **缓存机制**（4周）
   - 第1周：设计缓存架构
   - 第2周：实现 LRU 缓存
   - 第3周：实现磁盘存储层
   - 第4周：集成和测试

2. **复合索引优化**（4周）
   - 第1周：设计索引选择器
   - 第2周：实现索引选择逻辑
   - 第3周：收集统计信息
   - 第4周：集成到查询优化器

3. **索引统计信息**（4周）
   - 第1周：设计统计信息结构
   - 第2周：实现统计收集
   - 第3周：实现统计持久化
   - 第4周：集成和测试

### 5.3 长期实施（6-8个月）

**优先级：低**

1. **磁盘存储**（8周）
   - 第1-2周：设计存储架构
   - 第3-4周：实现 RocksDB 集成
   - 第5-6周：实现混合存储
   - 第7-8周：数据迁移和测试

2. **事务机制**（8周）
   - 第1-2周：设计事务架构
   - 第3-4周：实现事务管理器
   - 第5-6周：实现 MVCC
   - 第7-8周：集成和测试

## 六、总结

本设计方案从短期、中期、长期三个阶段提出了GraphDB索引系统的改进方案：

### 短期改进（1-2个月）
- 增加前缀查询支持，提升复合索引查询能力
- 优化并发控制，使用 DashMap 提升并发性能
- 完善持久化机制，支持索引数据的持久化和恢复

### 中期改进（3-4个月）
- 引入缓存机制，支持大规模数据处理
- 支持复合索引优化，提升查询性能
- 添加索引统计信息，支持智能索引选择

### 长期改进（6-8个月）
- 支持磁盘存储，突破内存限制
- 完善事务机制，保证数据一致性

所有改进均基于单节点架构，不涉及分布式复杂性，适合当前项目的发展方向。建议按优先级逐步实施，先完成短期改进，再根据实际需求决定是否实施中长期改进。
